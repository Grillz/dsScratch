{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *science* part of data science frequently involves forming and testing hypotheses about our data and the processes that generate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to test wether a certain hypothesis is true i.e. \"This coin is fair\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the classical setup we have a null hypothesis $H_{0}$ that represents som default position and some alternative hypothesis $H_{1}$ that we would like to compare it with\n",
    "    * We will use statistics to decide whether we can reject $H_{0}$ as false or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Example: Flipping a Coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we have a coin, and we want to test whether it's fair. We'll make the assumption that the coin has some probability *p* of landing heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our null hypothesis is that the coin is fair, p = 0.5, and we will test this against and alternative hypothesis p ≠ 0.5\n",
    "    * $H_{0}$:  p = 0.5\n",
    "    * $H_{1}$:  p ≠ 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our test will involve flipping the coin some number *n* times and conting the number of heads *X*. Each coin flip is a Bernoulli trial, which means that *X* is a Binomial(n,p) random variable, which we can aprroximate using the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n,p):\n",
    "    \"\"\"finds mu and sigma corresponding to a Binomial(n,p)\"\"\"\n",
    "    mu = n * p\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a random variable follows a normal distribution, we can use normal_cdf to figure our the probability that its realized value lies within(or outside) a particutlar interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_cdf(x, mu=0, sigma=1):\n",
    "    return (1 + math.erf((x - mu) /  math.sqrt(2) / sigma)) / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the normal cdf is the probability the variable is below a threshold\n",
    "normal_probability_below = normal_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it's above the threshold if it's not below\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)  #everything always equals one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it's between if it's less than hi, but not less than lo\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it's outside if it's not between\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also do the reverse -- find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of liklihood\n",
    "    * i.e. If we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% fo the probability (leaving 60%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will need inverse_normal_cdf from the previous chapter\n",
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    \"\"\"find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    \n",
    "    low_z, low_p = -10.0, 0            # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z,  hi_p  =  10.0, 1            # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the cdf's value there\n",
    "        if mid_p < p:\n",
    "            # midpoint is still too low, search above it\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # midpoint is still too high, search below it\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for which P(Z <= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for with P(Z >= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the symmetric (about the mean) bounds that contain the specified probability\"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    \n",
    "    #upper bound should have tail_probability about it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    #lower bound should have tail_probability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    return lower_bound, upper_bound    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In particular, let's say that we choose to flip the coin n= 1000 times. If our hypothesis of fairness is true, X should be distributed approximately normally with a mean 500 and standard deviatin 15.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n",
      "15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)\n",
    "print(mu_0)\n",
    "print(sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets graph this to make it more clear, we need normal_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu=0, sigma=1):\n",
    "    sqrt_two_pi = math.sqrt(2 * math.pi)\n",
    "    return (math.exp(-(x-mu)**2 / 2 / sigma ** 2) / (sqrt_two_pi * sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = [x/10 for x in range(4000, 6000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fabd0637128>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ikpiIhIPSUFERGp9/8BGFJNwBrIrGUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fabd0731278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs,[normal_pdf(x, mu_0, sigma_0) for x in xs], '-', label='mu=0,sigma=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to make a decision about significance -- how willing we are to make a *type 1 error* (false positive), in which we reject $H_{0}$ even though it's true. For reasons unknown, this willingness is often set at 5% or 1%. We will use 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider the test that rejects $H_{0}$ if *X* falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming *p* really equals 0.5 (i.e. $H_{0}$ is true), there is just a 5% chance we observe an *X* that lies outside this interval(469 - 530), which is the exact significance we wanted. Said another way, if $H_{0}$ is true, then, approximately 19 times out of 20, this test will give the correct result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are also often interested in the **power** of a test, which is the probability of not making a *type 2 error*, in which we fail to reject $H_{0}$ even though it's false. In order to measure this, we have to specify what exactly $H_{0}$ being false *means*. (Knowing merely that p is not 0.5 doesn't give you a ton of information about the distribution of *X*.) In particular, let's check what happens if *p* is really 0.55, so that the coin is slightly biased towards heads.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can calculate the power of the test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469.01026640487555\n",
      "530.9897335951244\n"
     ]
    }
   ],
   "source": [
    "# 95% bounds based on assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(lo)\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# actual mu and sigma based on p = 0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a type 2 error means we fail to reject the null hypothesis which will happen when X is still in our original interval\n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 -  type_2_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865480012953671\n"
     ]
    }
   ],
   "source": [
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Imagine instead that our null hypothesis was that the coin is not biased toward heads, or that p ≤ 0.5. In that case we want a *one-sided test* that rejects the null hypothesis when *X* is much larger than 50 but not when *X* is smaller than 50. So, a 5%- significance test involves using normal_probability_below to find the cutoff below which 95% of the probability lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.0073585242053\n"
     ]
    }
   ],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9363794803307173\n"
     ]
    }
   ],
   "source": [
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a more powerful test, since it no longer rejects $H_{0}$ when *X* is below 469 (which is very unlikely to happen if $H_{1}$ is true). ===p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An alternative way of thinking about the preceeding test involves *p-values*. Instead of choosing bounds based on some probability cutoff, we compute the probability -- assuming $H_{0}$ is true -- that we would see a value at least as extreme as the one we actually observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For our two-sided test of whether the coin is fair, we compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        # if x is greater than the mean, the tail is what's greater than x\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        #if x is less than the mean, the tail is what's less than x\n",
    "        return 2 * normal_probability_below(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we were to see 530 heads, we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598857"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(529.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, this tells us that we have a 6% chance of seeing this value, and we would not reject the null hypothesis because it is over 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why use 529.5 instead of 530? It's called continuity correction. It reflects the fact that normal_probability_between(529.5, 530.5, mu_0, sigma_0) is a better estimate of the probability of seeing 530 heads than normal_probability_between(530, 531, mu_0, sigma_0) is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One way to check that this is a reasonable estimate is with a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extreme_value_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 #count number of heads in 1000 flips\n",
    "                     for _ in range(1000))\n",
    "    \n",
    "    if num_heads >= 530 or num_heads <= 470:   #count how often the number is 'extreme'\n",
    "        extreme_value_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06162\n"
     ]
    }
   ],
   "source": [
    "print(extreme_value_count/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the p-value is greater than our 5% significance, we don't reject the null. If we instead saw 532 heads, the p-value would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which is smaller than the 5% significance, which means we would reject the null. It's the exact same test as before. It's just a different way of approaching the statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For our one-sided test if we saw 525 heads we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582083"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(524.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which means we wouldn't reject the null. If we saw 527 heads, the computation would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below 5% so we would reject the null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** * Before using normal_probability_above make sure your data is normally distributed, there are various statistical tests for normality, but even plotting the data is a good start**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been testing a hypothesis about the value of heads probability *p*, which is a *parameter* of the unknown \"heads\" distribution. When this is the case, there is another method we can use. We can construct a **confidence interval** around the observed value of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip- 1 if heads, 0 if tails. If we observe 525 heads out of 1,000 flips, then we estimate p equals 0.525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How confident can we be in this?\n",
    "    * If we know the exact value of *p*, the central limit theorem tells us that the average of those Bernoulli variables should be approximately normal, with a mean *p* and standard deviation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            math.sqrt(p * (1-p) / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, we don't know *p*, so instead we use our estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n",
      "0.015791611697353755\n"
     ]
    }
   ],
   "source": [
    "p_hat = 525/1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print(mu)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While this is not entirely justified, people will often do it anyway. Using the normal approximation, we conclude that we are \"95% confident\" that the following interval contains the true parameter *p*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is important to understand this statement. It is about the *interval* not about *p*. It should be understood as the asertion that if you were to repeat the experiment many times, 95% of the time the \"true\" parameter (which is the same every time) would lie within the observed confidence interval (which might be different every time)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note, that we do not conclude that the coin is unfair, since 0.5 falls within our confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To make the point, if we had seen 540 heads, then we'd have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.545\n",
      "0.015747221977225064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5141360300150551, 0.5758639699849449)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 545/1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print(mu)\n",
    "print(sigma)\n",
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, the fair coin doesn't lie within the confidence interval, and does not pass the 95% confidence test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###P-hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proceedure that erroneously rejects the null hypothesis only 5% of the time, will --by definition --5% of the time erroneously reject the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"flip a fair coin 1000 times, True = heads, False = tails\"\"\"\n",
    "    return [random.random() < 0.5 for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reject_fairness(experiment):\n",
    "    \"\"\"using the 5% significance levels\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "experiments = [run_experiment() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "num_rejections = len([experiment\n",
    "                     for experiment in experiments\n",
    "                     if reject_fairness(experiment)])\n",
    "print(num_rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 46 is the number of times we erroneously rejected the null hypothesis in an experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What all of this means is that if you're setting out to find \"significant\" results, you usually can. Test enough hypotheses against your data set, and one of them will almost certainly appear significant. You can remove the right outliers to prove your claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is often called **P-hacking**, if you want to do good *science*, **you should determine your hypothesis befor looking at the data, you should clean your data without the hypothesis in mind, and keep in mind that p-values are not substitutes for common sense**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Example: Running an A/B Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the primary responsibilities at DataSciencester is experience optimization, which is a euphemism for trying to get people to click on advertisements. One of your advertisers has developed a new energy drink targeted at data scientists, and the VP of Advertisements wants your help choosing between advertisement A \"tastes great\" and advertisement B \"less bias!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a scientist, you decide to run an experiment by randomly showing site visitors one of the two advertisements and tracking how many people click on each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see when it's a landslide, however if its a closer race statistical inference comes in to play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say that $N_{A}$ people see ad A, and that $n_{A}$ of them click it. We can think of each ad view as a Bernoulli trial where $p_{A}$ is the probability that someone clicks ad A. Then, if $N_{A}$ is large enough, we know that $\\frac{n_{A}}{N_{A}}$ is approximately a normal random variable with mean $p_{A}$ and a standard deviation $\\sigma_{A} = \\sqrt{\\frac{p_{A}(1 - p_{A})}{N_{A}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, we can have $n_{B}/N_{B}$ , with the same mean and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimated_parameters(N, n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p)/ N)\n",
    "    return p, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we assume those two normals are independent (which seems reasonable, since Bernoulli trials are),  then their difference should also be normal with mean $p_{B} - p_{A}$ and standard deviation $\\sqrt{\\sigma^2_{A} + \\sigma^2_{B}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* This is kindof cheating, the math only works out if we know the standard deviations. Here we're estimating them from the data, which means that we really should be using a *t-distribution*. However, for large datasets, it's close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can test the null hypothesis that $p_{A}$ and $p_{B}$ are the same or that $p_{A}$ - $p_{B}$ is zero using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A =  estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    \n",
    "    return (p_B - p_A)/ math.sqrt(sigma_A **2 + sigma_B **2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which should be approximately standard normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, if \"tastes great\" gets 200 clicks out of 1,000 views and \"less bias\" gets 180 clicks out of 1,000 views, the statistic equals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1403464899034472\n"
     ]
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000,200,1000,180)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The probability of seeing such a large difference if the means were actually equal would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A 25% chance of them being the same is large enought that you can't conclude there's much of a difference. On the other hand, if \"less bias\" only got 150 clicks, we'd have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000,200,1000,150)\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* which means there is only a .003 probability that you'd see suck a large difference if the ads were equally effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bayesian Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
