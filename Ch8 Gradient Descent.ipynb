{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in data science you are trying to solve some sort of optimization problem, and often you will have to solve them from scratch. Our approach to these problems is *gradient descent*, which lends itself pretty well to a from-scratch treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose we have some funcion **f** that takes as input a vector of real numbers and outputs a single real number. Pne simple such function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elemets in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Frequently we will need to maximize or minimize such functions. That is, we need to find the input **v** that produces the largest possible value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For functions like the one we are working on, the *gradient* (this is the vector of partial derivatives), gives the input direcction in which the function most quickly increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One apporach to maximizing a functino is to pick a random starting point, compute the gradient, take a small step in the direction of the gradient( ie the direction that causes the function to increase the most), and repeat with the new starting point. Similarly, you can try to minimize a function by taking small steps in the *opposite* direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* If a function has a unique global minimum, this procedure is likely to find it. If a function has multiple local minima, this procedure might \"find\" the worng one of them, in which case you might rerun the procedure from a variety of starting points. If a function has no minimum, thin it's possible the procedure might go on forever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Estimating the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **f** if a function of one variable, its derivative at a point **x**, measures how **f(x)** changes when we make a small change to **x**. It is defined as the limit of difference quotients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "#as h approches 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative is the slope of the tangent line at **(x, f(x))**, while the difference quotient is the slope of the not-quite-tangent line that runs through **(x + h, f(x + h))**. As *h* gets smaller and smaller, the not-quite-tangent line gets closer and closer to the tangent line. Here is an approximate photo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='gradient.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For a lot of functions it is pretty simple to calculate the derivative, such as the square function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if we couldn't find the gradient? You cannot take limits in Python (unless you use SymPy), however we can estimate the derivatives by evaluating the defference quotient for a very small *e*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1777ef3198>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntZhFXpJazCIvSS32/3VFYKspGgVFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1777ef32b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "x = range(-10,10)\n",
    "plt.plot(x, list(map(derivative, x)), 'rx')           # red  x\n",
    "plt.plot(x, list(map(derivative_estimate, x)), 'b+')\n",
    "#plt.legend(loc=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When **f** is a function of many variables, it has multiple *partial derivatives* each indicating how **f** changes when we make small changes in just one of the input variables\n",
    "* We calculate its ith partial derivative by treating it as a function of just its ith variable holding the other variables fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j==i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After that we can estimate the gradient the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* This method has the drawback of being computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Using the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to see that the **sum_of_squares** function is smallest when its input **v** is a vector of zeros. But imagin we didn't know that. Let's try and use gradients to find the minimum among all three-dimensional vectors. We'll just pick a random staring point and then take tiny steps in the opposite direction of the gradient until we reach a point where the gradient is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i in zip(v, direction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tolerance = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need our distance equation from the linear algebra chapter\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts the corresponding elements\"\"\"\n",
    "    return [v_i - w_i\n",
    "            for v_i, w_i in zip(v,w)]\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i\n",
    "               for v_i, w_i in zip(v,w))\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n \"\"\"\n",
    "    return dot(v, v)\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "def distance(v, w):\n",
    "    return magnitude(vector_subtract(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4, -4, 4]\n",
      "[-3.92, -3.92, 3.92]\n",
      "[-3.8416, -3.8416, 3.8416]\n",
      "[-3.764768, -3.764768, 3.764768]\n",
      "[-3.68947264, -3.68947264, 3.68947264]\n",
      "[-3.6156831872, -3.6156831872, 3.6156831872]\n",
      "[-3.543369523456, -3.543369523456, 3.543369523456]\n",
      "[-3.47250213298688, -3.47250213298688, 3.47250213298688]\n",
      "[-3.4030520903271424, -3.4030520903271424, 3.4030520903271424]\n",
      "[-3.3349910485205996, -3.3349910485205996, 3.3349910485205996]\n",
      "[-3.2682912275501876, -3.2682912275501876, 3.2682912275501876]\n",
      "[-3.202925402999184, -3.202925402999184, 3.202925402999184]\n",
      "[-3.1388668949392002, -3.1388668949392002, 3.1388668949392002]\n",
      "[-3.076089557040416, -3.076089557040416, 3.076089557040416]\n",
      "[-3.0145677658996077, -3.0145677658996077, 3.0145677658996077]\n",
      "[-2.9542764105816155, -2.9542764105816155, 2.9542764105816155]\n",
      "[-2.8951908823699832, -2.8951908823699832, 2.8951908823699832]\n",
      "[-2.8372870647225836, -2.8372870647225836, 2.8372870647225836]\n",
      "[-2.7805413234281318, -2.7805413234281318, 2.7805413234281318]\n",
      "[-2.724930496959569, -2.724930496959569, 2.724930496959569]\n",
      "[-2.670431887020378, -2.670431887020378, 2.670431887020378]\n",
      "[-2.61702324927997, -2.61702324927997, 2.61702324927997]\n",
      "[-2.5646827842943707, -2.5646827842943707, 2.5646827842943707]\n",
      "[-2.513389128608483, -2.513389128608483, 2.513389128608483]\n",
      "[-2.4631213460363135, -2.4631213460363135, 2.4631213460363135]\n",
      "[-2.413858919115587, -2.413858919115587, 2.413858919115587]\n",
      "[-2.3655817407332753, -2.3655817407332753, 2.3655817407332753]\n",
      "[-2.31827010591861, -2.31827010591861, 2.31827010591861]\n",
      "[-2.271904703800238, -2.271904703800238, 2.271904703800238]\n",
      "[-2.226466609724233, -2.226466609724233, 2.226466609724233]\n",
      "[-2.1819372775297485, -2.1819372775297485, 2.1819372775297485]\n",
      "[-2.1382985319791534, -2.1382985319791534, 2.1382985319791534]\n",
      "[-2.0955325613395703, -2.0955325613395703, 2.0955325613395703]\n",
      "[-2.053621910112779, -2.053621910112779, 2.053621910112779]\n",
      "[-2.0125494719105235, -2.0125494719105235, 2.0125494719105235]\n",
      "[-1.972298482472313, -1.972298482472313, 1.972298482472313]\n",
      "[-1.9328525128228669, -1.9328525128228669, 1.9328525128228669]\n",
      "[-1.8941954625664095, -1.8941954625664095, 1.8941954625664095]\n",
      "[-1.8563115533150814, -1.8563115533150814, 1.8563115533150814]\n",
      "[-1.8191853222487797, -1.8191853222487797, 1.8191853222487797]\n",
      "[-1.782801615803804, -1.782801615803804, 1.782801615803804]\n",
      "[-1.7471455834877279, -1.7471455834877279, 1.7471455834877279]\n",
      "[-1.7122026718179733, -1.7122026718179733, 1.7122026718179733]\n",
      "[-1.6779586183816138, -1.6779586183816138, 1.6779586183816138]\n",
      "[-1.6443994460139815, -1.6443994460139815, 1.6443994460139815]\n",
      "[-1.611511457093702, -1.611511457093702, 1.611511457093702]\n",
      "[-1.5792812279518278, -1.5792812279518278, 1.5792812279518278]\n",
      "[-1.5476956033927913, -1.5476956033927913, 1.5476956033927913]\n",
      "[-1.5167416913249354, -1.5167416913249354, 1.5167416913249354]\n",
      "[-1.4864068574984368, -1.4864068574984368, 1.4864068574984368]\n",
      "[-1.4566787203484681, -1.4566787203484681, 1.4566787203484681]\n",
      "[-1.4275451459414987, -1.4275451459414987, 1.4275451459414987]\n",
      "[-1.3989942430226687, -1.3989942430226687, 1.3989942430226687]\n",
      "[-1.3710143581622154, -1.3710143581622154, 1.3710143581622154]\n",
      "[-1.343594070998971, -1.343594070998971, 1.343594070998971]\n",
      "[-1.3167221895789918, -1.3167221895789918, 1.3167221895789918]\n",
      "[-1.290387745787412, -1.290387745787412, 1.290387745787412]\n",
      "[-1.2645799908716637, -1.2645799908716637, 1.2645799908716637]\n",
      "[-1.2392883910542305, -1.2392883910542305, 1.2392883910542305]\n",
      "[-1.214502623233146, -1.214502623233146, 1.214502623233146]\n",
      "[-1.190212570768483, -1.190212570768483, 1.190212570768483]\n",
      "[-1.1664083193531134, -1.1664083193531134, 1.1664083193531134]\n",
      "[-1.143080152966051, -1.143080152966051, 1.143080152966051]\n",
      "[-1.12021854990673, -1.12021854990673, 1.12021854990673]\n",
      "[-1.0978141789085956, -1.0978141789085956, 1.0978141789085956]\n",
      "[-1.0758578953304236, -1.0758578953304236, 1.0758578953304236]\n",
      "[-1.0543407374238152, -1.0543407374238152, 1.0543407374238152]\n",
      "[-1.0332539226753388, -1.0332539226753388, 1.0332539226753388]\n",
      "[-1.012588844221832, -1.012588844221832, 1.012588844221832]\n",
      "[-0.9923370673373954, -0.9923370673373954, 0.9923370673373954]\n",
      "[-0.9724903259906474, -0.9724903259906474, 0.9724903259906474]\n",
      "[-0.9530405194708345, -0.9530405194708345, 0.9530405194708345]\n",
      "[-0.9339797090814178, -0.9339797090814178, 0.9339797090814178]\n",
      "[-0.9153001148997895, -0.9153001148997895, 0.9153001148997895]\n",
      "[-0.8969941126017936, -0.8969941126017936, 0.8969941126017936]\n",
      "[-0.8790542303497577, -0.8790542303497577, 0.8790542303497577]\n",
      "[-0.8614731457427626, -0.8614731457427626, 0.8614731457427626]\n",
      "[-0.8442436828279073, -0.8442436828279073, 0.8442436828279073]\n",
      "[-0.8273588091713492, -0.8273588091713492, 0.8273588091713492]\n",
      "[-0.8108116329879222, -0.8108116329879222, 0.8108116329879222]\n",
      "[-0.7945954003281638, -0.7945954003281638, 0.7945954003281638]\n",
      "[-0.7787034923216005, -0.7787034923216005, 0.7787034923216005]\n",
      "[-0.7631294224751685, -0.7631294224751685, 0.7631294224751685]\n",
      "[-0.7478668340256651, -0.7478668340256651, 0.7478668340256651]\n",
      "[-0.7329094973451518, -0.7329094973451518, 0.7329094973451518]\n",
      "[-0.7182513073982487, -0.7182513073982487, 0.7182513073982487]\n",
      "[-0.7038862812502837, -0.7038862812502837, 0.7038862812502837]\n",
      "[-0.6898085556252781, -0.6898085556252781, 0.6898085556252781]\n",
      "[-0.6760123845127726, -0.6760123845127726, 0.6760123845127726]\n",
      "[-0.6624921368225171, -0.6624921368225171, 0.6624921368225171]\n",
      "[-0.6492422940860667, -0.6492422940860667, 0.6492422940860667]\n",
      "[-0.6362574482043454, -0.6362574482043454, 0.6362574482043454]\n",
      "[-0.6235322992402584, -0.6235322992402584, 0.6235322992402584]\n",
      "[-0.6110616532554533, -0.6110616532554533, 0.6110616532554533]\n",
      "[-0.5988404201903442, -0.5988404201903442, 0.5988404201903442]\n",
      "[-0.5868636117865373, -0.5868636117865373, 0.5868636117865373]\n",
      "[-0.5751263395508066, -0.5751263395508066, 0.5751263395508066]\n",
      "[-0.5636238127597905, -0.5636238127597905, 0.5636238127597905]\n",
      "[-0.5523513365045947, -0.5523513365045947, 0.5523513365045947]\n",
      "[-0.5413043097745027, -0.5413043097745027, 0.5413043097745027]\n",
      "[-0.5304782235790126, -0.5304782235790126, 0.5304782235790126]\n",
      "[-0.5198686591074324, -0.5198686591074324, 0.5198686591074324]\n",
      "[-0.5094712859252838, -0.5094712859252838, 0.5094712859252838]\n",
      "[-0.4992818602067781, -0.4992818602067781, 0.4992818602067781]\n",
      "[-0.4892962230026425, -0.4892962230026425, 0.4892962230026425]\n",
      "[-0.4795102985425897, -0.4795102985425897, 0.4795102985425897]\n",
      "[-0.4699200925717379, -0.4699200925717379, 0.4699200925717379]\n",
      "[-0.46052169072030313, -0.46052169072030313, 0.46052169072030313]\n",
      "[-0.4513112569058971, -0.4513112569058971, 0.4513112569058971]\n",
      "[-0.4422850317677791, -0.4422850317677791, 0.4422850317677791]\n",
      "[-0.43343933113242356, -0.43343933113242356, 0.43343933113242356]\n",
      "[-0.4247705445097751, -0.4247705445097751, 0.4247705445097751]\n",
      "[-0.4162751336195796, -0.4162751336195796, 0.4162751336195796]\n",
      "[-0.407949630947188, -0.407949630947188, 0.407949630947188]\n",
      "[-0.39979063832824424, -0.39979063832824424, 0.39979063832824424]\n",
      "[-0.3917948255616794, -0.3917948255616794, 0.3917948255616794]\n",
      "[-0.3839589290504458, -0.3839589290504458, 0.3839589290504458]\n",
      "[-0.3762797504694369, -0.3762797504694369, 0.3762797504694369]\n",
      "[-0.36875415546004814, -0.36875415546004814, 0.36875415546004814]\n",
      "[-0.36137907235084715, -0.36137907235084715, 0.36137907235084715]\n",
      "[-0.3541514909038302, -0.3541514909038302, 0.3541514909038302]\n",
      "[-0.34706846108575357, -0.34706846108575357, 0.34706846108575357]\n",
      "[-0.3401270918640385, -0.3401270918640385, 0.3401270918640385]\n",
      "[-0.3333245500267577, -0.3333245500267577, 0.3333245500267577]\n",
      "[-0.3266580590262225, -0.3266580590262225, 0.3266580590262225]\n",
      "[-0.3201248978456981, -0.3201248978456981, 0.3201248978456981]\n",
      "[-0.31372239988878414, -0.31372239988878414, 0.31372239988878414]\n",
      "[-0.3074479518910085, -0.3074479518910085, 0.3074479518910085]\n",
      "[-0.30129899285318834, -0.30129899285318834, 0.30129899285318834]\n",
      "[-0.2952730129961246, -0.2952730129961246, 0.2952730129961246]\n",
      "[-0.28936755273620207, -0.28936755273620207, 0.28936755273620207]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) #compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01)  #take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance: #stop if we're converging\n",
    "        break\n",
    "    print(v)  # Lets interactively see whats happening here\n",
    "    v = next_v        #continue if we're not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20114960951056973, -0.05028740237764243, -0.45258662139878214]\n"
     ]
    }
   ],
   "source": [
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The lower the tolerance the closer we will get to a vector of zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Choosing the Right Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rationale for moving against the gradient is clear, but choosing the right step size can be tricky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are several options for this:\n",
    "    * Using a fixed step size\n",
    "    * Gradually shrinking the step size over time\n",
    "    * At each step, choosing the step size that minimizes the value of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The last option is the most effective, but most computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We must be aware, that certain step sizes may cause invalid inputs for our function. To get around this we must return infinity for invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"Returns a new function that is the same as f, except when f produces an error it outpus infinity\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')     #this means infinity in python\n",
    "    \n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
